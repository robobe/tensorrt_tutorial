{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/user/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from albumentations import Resize, Compose\n",
    "from albumentations.pytorch.transforms import  ToTensorV2\n",
    "from albumentations.augmentations.transforms import Normalize\n",
    " \n",
    "def preprocess_image(img_path):\n",
    "    # transformations for the input data\n",
    "    transforms = Compose([\n",
    "        Resize(224, 224, interpolation=cv2.INTER_NEAREST),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "     \n",
    "    # read input image\n",
    "    input_img = cv2.imread(img_path)\n",
    "    # do transformations\n",
    "    input_data = transforms(image=input_img)[\"image\"]\n",
    "    batch_data = torch.unsqueeze(input_data, 0)\n",
    "    return batch_data\n",
    " \n",
    "input = preprocess_image(\"turkish_coffee.jpg\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.cuda()\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: cup , confidence: 92.43077850341797 %, index: 968\n",
      "class: espresso , confidence: 6.13804292678833 %, index: 967\n",
      "class: coffee mug , confidence: 0.7285578846931458 %, index: 504\n"
     ]
    }
   ],
   "source": [
    "def postprocess(output_data):\n",
    "    # get class names\n",
    "    with open(\"imagenet_classes.txt\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    # calculate human-readable value by softmax\n",
    "    confidences = torch.nn.functional.softmax(output_data, dim=1)[0] * 100\n",
    "    # find top predicted classes\n",
    "    _, indices = torch.sort(output_data, descending=True)\n",
    "    i = 0\n",
    "    # print the top classes predicted by the model\n",
    "    while confidences[indices[0][i]] > 0.5:\n",
    "        class_idx = indices[0][i]\n",
    "        print(\n",
    "            \"class:\",\n",
    "            classes[class_idx],\n",
    "            \", confidence:\",\n",
    "            confidences[class_idx].item(),\n",
    "            \"%, index:\",\n",
    "            class_idx.item(),\n",
    "        )\n",
    "        i += 1\n",
    " \n",
    "postprocess(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ONNX_FILE_PATH = 'resnet50.onnx'\n",
    "torch.onnx.export(model, input, ONNX_FILE_PATH, input_names=['input'],\n",
    "                  output_names=['output'], export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(ONNX_FILE_PATH)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    " \n",
    "# logger to capture errors, warnings, and other information during the build and inference phases\n",
    "TRT_LOGGER = trt.Logger()\n",
    " \n",
    "def build_engine(onnx_file_path):\n",
    "    # initialize TensorRT engine and parse ONNX model\n",
    "    builder = trt.Builder(TRT_LOGGER)\n",
    "    network = builder.create_network()\n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "     \n",
    "    # parse ONNX\n",
    "    with open(onnx_file_path, 'rb') as model:\n",
    "        print('Beginning ONNX file parsing')\n",
    "        parser.parse(model.read())\n",
    "    print('Completed parsing of ONNX file')\n",
    "# allow TensorRT to use up to 1GB of GPU memory for tactic selection\n",
    "builder.max_workspace_size = 1 << 30\n",
    "# we have only one image in batch\n",
    "builder.max_batch_size = 1\n",
    "# use FP16 mode if possible\n",
    "if builder.platform_has_fast_fp16:\n",
    "    builder.fp16_mode = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
